{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Regression Analysis\n",
    "\n",
    "Form of predictive modelling technique which investigates the relationship between a dependent (target) and independent (predictor) variables. Can be used for forecasting, time series modelling, and finding the causal effect relationship between variables.\n",
    "\n",
    "# Why do we use it?\n",
    "\n",
    "It can indicate the significant relationships between dependent and indepent variables\n",
    "\n",
    "It indicates the strength of impact of multiple independent variables on a dependent variable.\n",
    "\n",
    "Also allows us to compare the effects of variables measured on different scales ie. effect of price changes and the number of promotional activities.\n",
    "\n",
    "These benefits help to eliminate and evaluate the best set of variables to be used for building predictive models.\n",
    "\n",
    "# 1. Linear Regression\n",
    "\n",
    "Establishes a relationship between dependent variable (y) and one or more independent variables (X) using a BEST FIT STRAIGHT LINE (AKA Regression line)\n",
    "\n",
    "Equation:\n",
    "y = a + bx + 3\n",
    "\n",
    "Where a is the intercept, b is the slope of the line, and e is the error term.\n",
    "\n",
    "The Difference between a simple linear regression and multiple linear regression is that, multiple linear regression has (>1) independent variables, whereas simple linear regression has only 1 independent variable.\n",
    "\n",
    "How do we obtain a best fit line?\n",
    "By using the least square method, which calculates the best fit line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line\n",
    "\n",
    "We can then evaluate the model performance by using the metric R-square\n",
    "\n",
    "IMPORT POINTS\n",
    "- There must be a linear relationship between independent and dependent variables.\n",
    "- Multiple regression may suffer from multicollinearity, autocorrelation, heteroskedasticity\n",
    "- Linear Regression is very sensitive to outliers which can terribly affect the regression line and eventually the forecasted values.\n",
    "- Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is that the coefficient estimates are unstable.\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "Used to find the probability of event-Success and event-Failure - ie. the dependent variable is binary in nature. The parameters are chosen to maximize the likelihood of observing the sample values rather than minimizing the sum of squared errors like in ordinary regression.\n",
    "\n",
    "IMPORT POINTS\n",
    "- Widely used for classification problems\n",
    "- Doesn't require linear relationships between dependent and independent variables. It can handle various types of relationships because it applies a non-linear log transformation to the predicted odds ratio\n",
    "- To avoid over fitting and under fitting, we should include all significant variables. A good approach to ensure this practice is to use a step wise method to estimate the logistic regression.\n",
    "- It requires LARGE SAMPLE SIZES because maximum likelihood estimates are less powerful at low samples sizes than ordinary least square\n",
    "- The independent variables should not be correlated with each other ie. NO MULTICOLLINEARITY. However, we have the options to include interaction effects of categorical variables in the analysis and in the model.\n",
    "- If the values of dependent variables is ordinal, then it is called as Ordinal logistic regression\n",
    "- If dependent variable is multi class then it is known as Multinomial Logistic Regression\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "A regression equation is a polynomial regression equation if the power of the independent variable is more than 1\n",
    "\n",
    "Equation:\n",
    "y = a + bx^2\n",
    "\n",
    "In regression technique, the best fit line is not a straight line but rather a curve that fits into the data points.\n",
    "\n",
    "IMPORTANT POINTS\n",
    "- While it may be tempting to fit a higher degree polynomial to get lower error, this can result in over-fitting.\n",
    "- Always plot the relationships to see the fit and focus on making sure that the curve fits the nature of the problem\n",
    "- Especially look out for  curve towards the end and see whether those shapes and trends make sense. Higher polynomials can end up producing weird results on extrapolation.\n",
    "\n",
    "# Stepwise Regression\n",
    "\n",
    "This form of regression is used when we deal with multiple independent variables. In this technique, the selection of independent variables is done with the help of an automatic process which involves NO HUMAN INTERVENTION\n",
    "\n",
    "This is done by observing statistical values like R-square, t-stats, and AIC metric to discern significant variables.\n",
    "\n",
    "Basically fits the regression model by adding/dropping co-variates one at a time based on specified criterion. \n",
    "\n",
    "Standard Stepwise Regression\n",
    "- adds and removes predictors as needed for each step\n",
    "\n",
    "Forward Selection\n",
    "- starts with the most significant predictor in the model and adds a variable for each step\n",
    "\n",
    "Backward elimination\n",
    "- starts with all predictors in the model and removes the least significant variable in each step\n",
    "\n",
    "The aim of this modeling technique is to maximize the prediction power with minimum number of predictor variables. It is one of the methods used to handle higher dimensionality of a data set.\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "Technique used when data suffers from multicollinearity (independent variables are highly correlated).\n",
    "\n",
    "In multicollinearity, even though the least squares estimates (OLS) are unbiased, their variances are large which deviates the observed value far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n",
    "\n",
    "Equation:\n",
    "y = a + b1x1 + b2x2 + ... + e, for multiple independent variables where e is the error term/value needed to correct for a prediction error between the observed and predicted value\n",
    "\n",
    "In a linear equation, prediction errors can be decomposed into two sub components. First is due to bias and the second due to variance. Prediction error can occur due to any one of these two or both components. \n",
    "\n",
    "Ridge regression solves the multicollinearity problem through shrinkage parameter\n",
    "\n",
    "IMPORTANT POINTS\n",
    "- The assumptions of this regression is the same as least squared regression except normality is not to be assumed.\n",
    "- It shrinks the value of coefficients but doesn't reach zero, which suggests NO FEATURE SELECTION FEATURE\n",
    "- This is a regularization method and uses l2 regularization\n",
    "\n",
    "# Lasso Regression\n",
    "\n",
    "Similar to Ridge Regression, Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients. In addition, it is capable of reducing the variability and improving the accuracy of linear regression models. \n",
    "\n",
    "Lasso Regression DIFFERS from ridge regression in a way that it uses absolute values in the penalty function, instead of squares. This leads to penalizing (or equivalently constraining the sum of the absolute values of the estimates) values which causes some of the parameter estimates to turn out exactly zero. The larger the penalty applied, the further the estimates get shrunk towards absolute zero. This results to variable selection of of given n variables\n",
    "\n",
    "IMPORTANT POINTS\n",
    "- The assumptions of this regression are the same as least squared regression except normality is not to be assumed.\n",
    "- It shrinks coefficients to zero (exactly zero) which helps in feature selection\n",
    "- This is a regularization method and uses l1 regularization\n",
    "- If a group of predictors are highly correlated, lasso picks only one of them and shrinks the others to zero\n",
    "\n",
    "# ElasticNet Regression\n",
    "\n",
    "Hybrid of Lasso and Ridge Regression Techniques. It is trained with L1 and L2 prior as a regularizer.\n",
    "\n",
    "Useful when there are multiple features which are correlated. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both\n",
    "\n",
    "A practical advantage of trading off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge's stability under rotation\n",
    "\n",
    "IMPORTANT POINTS\n",
    "- Encourages group effect in case of highly correlated variables\n",
    "- There are no limitations on the number of selected variables\n",
    "- It can suffer with double shrinkage\n",
    "\n",
    "# Other Models to explore\n",
    "Bayesian Regression\n",
    "Ecological Regression\n",
    "Robust Regression\n",
    "\n",
    "# How to select the right regression model?\n",
    "\n",
    "Below are Key Factors that you should practice to select the right regression model\n",
    "\n",
    "1. Data Exploration should always be the first step before selecting the right model to identify the relationship and impact of variables\n",
    "\n",
    "2. To compare the goodness of fit for different models, we can analyse different metrics like statistical significance of parameters, R-square, Adjusted r-quare, AIC, BIC, and error term.\n",
    "\n",
    "3. CROSS-VALIDATION is the best way to evaluate models used for prediction. Here you divide your data set into two groups (training and testing). A simple mean squared difference between the observed and predicted values gives you a measure for prediction accuracy\n",
    "\n",
    "4. If your data set has multiple confounding variables, you should NOT choose an automatic model selection method because you do not want to put these in a model at the same time.\n",
    "\n",
    "5. It'll also depend on your objective. It can occur that a less powerful model is easy to implement as compared to a highly statistically significant model.\n",
    "\n",
    "6. Regression regularization methods (Lasso, Ridge, and ElasticNet) works well in case of high dimensionality and multicollinearity among the variables in the data set.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
