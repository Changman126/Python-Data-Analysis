{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named zope.interface",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-113f207fbcbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspiders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCrawlSpider\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinkextractors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinkExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mExampleSpider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCrawlSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ochang\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scrapy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;31m# Apply monkey patches to fix issues in external libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_monkeypatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0m_monkeypatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ochang\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scrapy\\_monkeypatches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;31m# Undo what Twisted's perspective broker adds to pickle register\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m# to prevent bugs like Twisted#7989 while serializing requests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersisted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyles\u001b[0m  \u001b[1;31m# NOQA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[1;31m# Remove only entries with twisted serializers for non-twisted types.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopyreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ochang\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\twisted\\persisted\\styles.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;31m# Twisted Imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreflect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ochang\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\twisted\\python\\log.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mzope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterface\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtwisted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_PY3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named zope.interface"
     ]
    }
   ],
   "source": [
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "class ExampleSpider(CrawlSpider):\n",
    "    name = \"course_scraper\"\n",
    "    allowed_domains = [\"www.coursera.org\"] # Which (sub-)domains shall be scraped?\n",
    "    start_urls = [\"https://www.coursera.org\"] # Start with this one\n",
    " \n",
    "    # Follow any link scrapy finds (that is allowed and matches the patterns).\n",
    "    rules = [Rule(LinkExtractor(allow=(r'/specializations.*', r'/learn.*'),deny=(r'.*authMode.*')), callback='parse_item', follow=True)] \n",
    "    #rules = [Rule(LinkExtractor(allow=(r'/specializations.*'),deny=(r'.*authMode.*')), callback='parse_item', follow=True)] \n",
    "\n",
    "    def parse_item(self, response):\n",
    "        \n",
    "        if 'specialization' in response.url:\n",
    "            title = response.css('h1 span::text').extract_first()\n",
    "        else:\n",
    "            title = response.css('h1::text').extract_first()\n",
    "\n",
    "        if len(response.css('div.body-1-text span::text').extract()) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            yield {\n",
    "                'title': title,\n",
    "                'url': response.url,\n",
    "                'text': response.css('div.body-1-text span::text').extract(),\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
